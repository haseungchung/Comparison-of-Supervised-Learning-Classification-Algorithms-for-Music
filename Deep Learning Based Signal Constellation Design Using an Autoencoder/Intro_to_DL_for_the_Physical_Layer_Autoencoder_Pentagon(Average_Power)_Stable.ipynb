{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haseungchung/Machine-Learning-and-Deep-Learning-Projects/blob/main/Deep%20Learning%20Based%20Signal%20Constellation%20Design%20Using%20an%20Autoencoder/Intro_to_DL_for_the_Physical_Layer_Autoencoder_Pentagon(Average_Power)_Stable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip uninstall keras\n",
        "# !pip install tensorflow-gpu==2.0.0\n",
        "# !pip install keras==2.3.1\n",
        "print(keras.__version__)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VedUq1_Qy4dK",
        "outputId": "01e52e34-4900-49c9-e7c1-246cc8d729b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1\n",
            "2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl.reader.excel import load_workbook\n",
        "from keras.layers import Input, Dense, GaussianNoise,Lambda,Dropout,Add,Concatenate\n",
        "from keras.models import Model\n",
        "from keras.constraints import max_norm\n",
        "from keras import regularizers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras import backend as K\n",
        "from google.colab import files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaNgNn8L4Yf9",
        "outputId": "80dd4ea9-81b6-440d-a3ba-abf1151348ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3lYmlIiBk1cD"
      },
      "outputs": [],
      "source": [
        "# for reproducing result\n",
        "tf.random.set_seed(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxb5fCbTk1cG",
        "outputId": "8106f987-7841-4694-cc1b-7c6e41e0895a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M: 16 k: 4 n: 2\n"
          ]
        }
      ],
      "source": [
        "# defining parameters\n",
        "# define (n,k) here for (n,k) autoencoder\n",
        "# n = n_channel \n",
        "# k = log2(M)  ==> so for (7,4) autoencoder n_channel = 7 and M = 2^4 = 16 \n",
        "M = 16\n",
        "k = np.log2(M)\n",
        "k = int(k)\n",
        "n_channel = 2\n",
        "R = k/n_channel\n",
        "max_val=1\n",
        "print ('M:',M,'k:',k,'n:',n_channel)\n",
        "prop=300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oodKZ5rLk1cM"
      },
      "outputs": [],
      "source": [
        "#generating data of size N\n",
        "N = 75000\n",
        "label = np.random.randint(M,size=N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L68sasY0k1cR"
      },
      "outputs": [],
      "source": [
        "# creating one hot encoded vectors\n",
        "data=np.zeros([N,M])\n",
        "for i in range(len(label)):\n",
        "    data[i][label[i]] = 1\n",
        "\n",
        "\n",
        "EbNo_train = 5.01187 #  coverted 7 db of EbNo\n",
        "noise=np.random.normal(0,np.sqrt(1/(2*R*EbNo_train)),[N,n_channel])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGSiDc0Mk1cd",
        "outputId": "c85085f8-3ad7-4b0a-f2ef-f58e007c3ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 16)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 16)           272         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            34          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 2)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 2)            8           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 2)            0           input_2[0][0]                    \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 16)           48          add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 16)           272         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 634\n",
            "Trainable params: 630\n",
            "Non-trainable params: 4\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "''' a custom activation function for amplitude constraint (~QAM style)'''\n",
        "def mapping_to_target_range( x, target_min=-np.sqrt(max_val), target_max=np.sqrt(max_val) ) :\n",
        "    x02 = K.tanh(x)+1 # x in range(0,2)\n",
        "    scale = ( target_max-target_min )/2.\n",
        "    return  x02 * scale + target_min\n",
        "\n",
        "'''a custom loss function for the average power constraint (~unique result)'''\n",
        "def custom_loss(signal):\n",
        "    def loss(y_true, y_pred):\n",
        "        sloss = K.switch(K.greater((K.sum(K.square(signal))),K.constant(max_val)),K.sum(K.square(signal))-K.constant(max_val),K.constant(0))\n",
        "        return sloss/prop+keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "# defining autoencoder and its layer\n",
        "input_signal = Input(shape=(M,))\n",
        "encoded = Dense(M, activation='relu')(input_signal)\n",
        "encoded1 = Dense(n_channel, activation='linear')(encoded)\n",
        "encoded2 = BatchNormalization()(encoded1) #average power\n",
        "# encoded2 = Dense(n_channel, activation=mapping_to_target_range)(encoded1) #QAM style\n",
        "''' an activation function for energy constraint (~PSK style)'''\n",
        "# encoded2 = Lambda(lambda x: np.sqrt(n_channel)*K.l2_normalize(x,axis=1))(encoded1) #only implement for the energy constraint\n",
        "\n",
        "inputnoise = Input(shape=(n_channel,))\n",
        "encoded3 = Add()([inputnoise,encoded2])\n",
        "\n",
        "decoded = Dense(M, activation='relu')(encoded3)\n",
        "decoded1 = Dense(M, activation='softmax')(decoded)\n",
        "autoencoder = Model([input_signal,inputnoise], decoded1)\n",
        "adam = Adam(learning_rate=0.005)\n",
        "autoencoder.compile(optimizer=adam, loss=custom_loss(encoded2)) #only implement for average power constraint\n",
        "# autoencoder.compile(optimizer=adam, loss='categorical_crossentropy') #loss for every other constraint\n",
        "# autoencoder summary\n",
        "print (autoencoder.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29wApxngk1cr",
        "scrolled": true,
        "outputId": "a3e5dfad-26b6-4ca7-c566-69fa50bfb7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 75000 samples, validate on 1500 samples\n",
            "Epoch 1/75\n",
            "75000/75000 [==============================] - 2s 29us/step - loss: 1.1396 - val_loss: 0.7672\n",
            "Epoch 2/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8380 - val_loss: 0.7409\n",
            "Epoch 3/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8300 - val_loss: 0.7287\n",
            "Epoch 4/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8211 - val_loss: 0.7482\n",
            "Epoch 5/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8164 - val_loss: 0.7099\n",
            "Epoch 6/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8167 - val_loss: 0.7046\n",
            "Epoch 7/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8129 - val_loss: 0.7095\n",
            "Epoch 8/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8211 - val_loss: 0.6991\n",
            "Epoch 9/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8157 - val_loss: 0.6990\n",
            "Epoch 10/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8158 - val_loss: 0.7010\n",
            "Epoch 11/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8213 - val_loss: 0.7115\n",
            "Epoch 12/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8229 - val_loss: 0.7023\n",
            "Epoch 13/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8216 - val_loss: 0.7000\n",
            "Epoch 14/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8118 - val_loss: 0.7078\n",
            "Epoch 15/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8229 - val_loss: 0.6988\n",
            "Epoch 16/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8082 - val_loss: 0.7048\n",
            "Epoch 17/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8127 - val_loss: 0.7022\n",
            "Epoch 18/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8246 - val_loss: 0.7098\n",
            "Epoch 19/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8216 - val_loss: 0.7086\n",
            "Epoch 20/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8121 - val_loss: 0.6936\n",
            "Epoch 21/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8093 - val_loss: 0.7119\n",
            "Epoch 22/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8164 - val_loss: 0.7030\n",
            "Epoch 23/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8162 - val_loss: 0.6952\n",
            "Epoch 24/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8141 - val_loss: 0.6982\n",
            "Epoch 25/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8155 - val_loss: 0.6999\n",
            "Epoch 26/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8112 - val_loss: 0.7055\n",
            "Epoch 27/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8141 - val_loss: 0.6967\n",
            "Epoch 28/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8199 - val_loss: 0.7003\n",
            "Epoch 29/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8159 - val_loss: 0.6966\n",
            "Epoch 30/75\n",
            "75000/75000 [==============================] - 3s 36us/step - loss: 0.8125 - val_loss: 0.6971\n",
            "Epoch 31/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8110 - val_loss: 0.6958\n",
            "Epoch 32/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8154 - val_loss: 0.6960\n",
            "Epoch 33/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8160 - val_loss: 0.6917\n",
            "Epoch 34/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8181 - val_loss: 0.6986\n",
            "Epoch 35/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8171 - val_loss: 0.7003\n",
            "Epoch 36/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8020 - val_loss: 0.6904\n",
            "Epoch 37/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8132 - val_loss: 0.6930\n",
            "Epoch 38/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8065 - val_loss: 0.6921\n",
            "Epoch 39/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8097 - val_loss: 0.6958\n",
            "Epoch 40/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8180 - val_loss: 0.7018\n",
            "Epoch 41/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8165 - val_loss: 0.6969\n",
            "Epoch 42/75\n",
            "75000/75000 [==============================] - 2s 28us/step - loss: 0.8068 - val_loss: 0.7012\n",
            "Epoch 43/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8118 - val_loss: 0.7060\n",
            "Epoch 44/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8185 - val_loss: 0.7007\n",
            "Epoch 45/75\n",
            "75000/75000 [==============================] - 2s 28us/step - loss: 0.8144 - val_loss: 0.6950\n",
            "Epoch 46/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8159 - val_loss: 0.7100\n",
            "Epoch 47/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8130 - val_loss: 0.6958\n",
            "Epoch 48/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8101 - val_loss: 0.6953\n",
            "Epoch 49/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8130 - val_loss: 0.7018\n",
            "Epoch 50/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8063 - val_loss: 0.6924\n",
            "Epoch 51/75\n",
            "75000/75000 [==============================] - 2s 30us/step - loss: 0.8131 - val_loss: 0.6903\n",
            "Epoch 52/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8159 - val_loss: 0.7002\n",
            "Epoch 53/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8045 - val_loss: 0.6888\n",
            "Epoch 54/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8095 - val_loss: 0.6934\n",
            "Epoch 55/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8063 - val_loss: 0.6994\n",
            "Epoch 56/75\n",
            "75000/75000 [==============================] - 2s 28us/step - loss: 0.8080 - val_loss: 0.6972\n",
            "Epoch 57/75\n",
            "75000/75000 [==============================] - 2s 28us/step - loss: 0.8136 - val_loss: 0.6989\n",
            "Epoch 58/75\n",
            "75000/75000 [==============================] - 2s 31us/step - loss: 0.8087 - val_loss: 0.7047\n",
            "Epoch 59/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8110 - val_loss: 0.6939\n",
            "Epoch 60/75\n",
            "75000/75000 [==============================] - 2s 29us/step - loss: 0.8082 - val_loss: 0.7026\n",
            "Epoch 61/75\n",
            "75000/75000 [==============================] - 2s 28us/step - loss: 0.8116 - val_loss: 0.7042\n",
            "Epoch 62/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8137 - val_loss: 0.7133\n",
            "Epoch 63/75\n",
            "75000/75000 [==============================] - 2s 27us/step - loss: 0.8097 - val_loss: 0.6933\n",
            "Epoch 64/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8105 - val_loss: 0.6952\n",
            "Epoch 65/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8152 - val_loss: 0.6988\n",
            "Epoch 66/75\n",
            "75000/75000 [==============================] - 2s 28us/step - loss: 0.8202 - val_loss: 0.6986\n",
            "Epoch 67/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8217 - val_loss: 0.6992\n",
            "Epoch 68/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8044 - val_loss: 0.7052\n",
            "Epoch 69/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8101 - val_loss: 0.6908\n",
            "Epoch 70/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8107 - val_loss: 0.6952\n",
            "Epoch 71/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8103 - val_loss: 0.6990\n",
            "Epoch 72/75\n",
            "75000/75000 [==============================] - 2s 26us/step - loss: 0.8101 - val_loss: 0.6920\n",
            "Epoch 73/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8165 - val_loss: 0.6942\n",
            "Epoch 74/75\n",
            "75000/75000 [==============================] - 2s 25us/step - loss: 0.8131 - val_loss: 0.6907\n",
            "Epoch 75/75\n",
            "75000/75000 [==============================] - 3s 40us/step - loss: 0.8119 - val_loss: 0.6954\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9e3944f4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# traning the autoencoder\n",
        "\n",
        "autoencoder.fit([data,noise], data,\n",
        "                epochs=75,\n",
        "                batch_size=int(M*5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pDc4QGu_k1c2"
      },
      "outputs": [],
      "source": [
        "# isolating the encoder section from full autoencoder\n",
        "encoder = Model([input_signal,inputnoise], encoded3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ufDQ_9vJk1c8"
      },
      "outputs": [],
      "source": [
        "# isolating the decoder section from full autoencoder\n",
        "encoded_input = Input(shape=(n_channel,))\n",
        "\n",
        "deco = autoencoder.layers[-2](encoded_input)\n",
        "deco = autoencoder.layers[-1](deco)\n",
        "decoder = Model(encoded_input, deco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jV7rmDJfk1dA"
      },
      "outputs": [],
      "source": [
        "# Making test data\n",
        "Ntest = 50000\n",
        "test_label = np.random.randint(M,size=Ntest)\n",
        "test_data=np.zeros([Ntest,M])\n",
        "for i in range(len(test_label)):\n",
        "    test_data[i][label[i]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRzXGCbIk1dG",
        "outputId": "4496e943-e808-4e4a-dbb8-f4746d2d4a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 1, 2)\n",
            "(16, 1, 2)\n"
          ]
        }
      ],
      "source": [
        "# for plotting learned constallation diagram\n",
        "scatter_plot = []\n",
        "scatter_plot_nn = []\n",
        "for i in range(0,M):\n",
        "    temp = np.zeros(M)\n",
        "    temp[i] = 1\n",
        "    scatter_plot.append(encoder.predict([np.expand_dims(temp,axis=0),np.random.normal(0,np.sqrt(1/(2*R*EbNo_train)),[1,2])]))\n",
        "    scatter_plot_nn.append(encoder.predict([np.expand_dims(temp,axis=0),np.random.normal(0,0.01,[1,2])]))\n",
        "    # print(np.expand_dims(temp,axis=0).shape)\n",
        "scatter_plot = np.array(scatter_plot)\n",
        "scatter_plot_nn = np.array(scatter_plot_nn)\n",
        "print(scatter_plot.shape)\n",
        "print(scatter_plot_nn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "Q2yI90l6k1dP",
        "outputId": "abb3e078-def5-4545-d068-70aa7e234a15"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# # plotting constellation diagram\n",
        "import matplotlib.pyplot as plt\n",
        "'''Average Power Constraint'''\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.title(\"Constellation Map of Signals\\n Under Average Power Constraint\")\n",
        "scatter_plot_nn = scatter_plot_nn.reshape(M,2,1)\n",
        "plt.scatter(scatter_plot_nn[:,0],scatter_plot_nn[:,1])\n",
        "plt.axis((-2.5,2.5,-2.5,2.5))\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TGYWwqdYElb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "Intro to DL for the Physical Layer_Autoencoder_Pentagon(Average Power)_Stable.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}